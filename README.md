# Awesome Attention üß†üî•

A curated and well-documented PyTorch implementation of modern **attention mechanisms**, designed for learning, research, and real-world applications.

> ‚≠ê Focused, Minimal, and Extensible ‚Äî perfect for understanding the core logic behind various attention variants.

## üöÄ Overview

This repository provides clean and efficient implementations of popular **attention mechanisms** in PyTorch. It aims to serve as both a learning resource and a foundation for research and production.

### ‚úÖ Implemented Modules

| Module Name                | Description                                                      |
|---------------------------|------------------------------------------------------------------|
| `MultiHeadAttention`      | Standard scaled dot-product multi-head attention (Vaswani et al.) |
| `MultiQueryAttention`     | Uses one query head and multiple key-value heads (Shazeer et al.) |
| `GroupedQueryAttention`   | A hybrid form where groups of query heads share key-value heads   |
| *(More coming soon!)*     | Long-range attention, sparse attention, flash attention, etc.     |

---

## üì¶ Features

- ‚úÖ **Readable and modular** PyTorch code  
- ‚úÖ Works with both **float32 / bfloat16 / float16**  
- ‚úÖ Simple interface, **plug-and-play** with your models  
- ‚úÖ Designed for **easy extension** and customization  
- ‚úÖ Clean gradient flow and proper masking support  
- ‚úÖ Ideal for both **educational** and **experimental** use

---

## üß© Example Usage

```python
from attention import GroupedQueryAttention

attn = GroupedQueryAttention(
    d_model=512,
    n_heads=8,
    n_groups=4,
)

x = torch.randn(2, 128, 512)  # [batch, seq_len, d_model]
out = attn(x)                 # [batch, seq_len, d_model]
