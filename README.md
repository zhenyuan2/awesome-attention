<h1 align="center">Awesome Attention ğŸ§ ğŸ”¥</h1>

A curated and well-documented PyTorch implementation of modern **attention mechanisms**, designed for learning, research, and real-world applications.

> â­ Focused, Minimal, and Extensible â€” perfect for understanding the core logic behind various attention variants.

## ğŸš€ Overview

This repository provides clean and efficient implementations of popular **attention mechanisms** in PyTorch. It aims to serve as both a learning resource and a foundation for research and production.

### âœ… Implemented Modules

| Module Name                | Description                                                      |
|---------------------------|------------------------------------------------------------------|
| `MultiHeadAttention`      | Standard scaled dot-product multi-head attention (Vaswani et al.) |
| `MultiQueryAttention`     | Uses one query head and multiple key-value heads (Shazeer et al.) |
| `GroupedQueryAttention`   | A hybrid form where groups of query heads share key-value heads   |
| *(More coming soon!)*     | Long-range attention, sparse attention, flash attention, etc.     |

---

## ğŸ“¦ Features

- âœ… **Readable and modular** PyTorch code  
- âœ… Works with both **float32 / bfloat16 / float16**  
- âœ… Simple interface, **plug-and-play** with your models  
- âœ… Designed for **easy extension** and customization  
- âœ… Clean gradient flow and proper masking support  
- âœ… Ideal for both **educational** and **experimental** use

---

## ğŸ§© Example Usage

```python
from attention import GroupedQueryAttention

attn = GroupedQueryAttention(
    d_model=512,
    n_heads=8,
    n_groups=4,
)

x = torch.randn(2, 128, 512)  # [batch, seq_len, d_model]
out = attn(x)                 # [batch, seq_len, d_model]
